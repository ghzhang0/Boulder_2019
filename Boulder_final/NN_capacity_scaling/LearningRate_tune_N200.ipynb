{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in file autograd/two_layer_net_autograd.py\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_exps = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "N = 200 # input dimension and number of training behaviors\n",
    "M = N # output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nonlinear activation function\n",
    "def f(x): # nonlinear conversion function to binary\n",
    "    return x.sigmoid()#tanh().add(1).div(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input data\n",
    "x = torch.tensor(np.genfromtxt(\"x_{}.csv\".format(N), delimiter=','), device=device).float()\n",
    "y = torch.tensor(np.genfromtxt(\"y_{}.csv\".format(N), delimiter=','), device=device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find optimal learning rates for each R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine lrs_N200 = [2500, 1000, 650, 100, 100, 100, 100, 100, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_all_lr = []\n",
    "for learning_rate in [2500]: # try different learning rates for different R and look at behavior of loss\n",
    "    R = int(N**1) # \n",
    "    # Create random Tensors for weights; setting requires_grad=True means that we\n",
    "    # want to compute gradients for these Tensors during the backward pass.\n",
    "    w1 = torch.randn(N, R, device=device, requires_grad=True).float()\n",
    "    w2 = torch.randn(R, M, device=device, requires_grad=True).float()\n",
    "\n",
    "    ## initialize tensor variables for bias terms \n",
    "    b1 = torch.randn(1, R, device=device, requires_grad=True).float()# bias for hidden layer\n",
    "    b2 = torch.randn(1, M, device=device, requires_grad=True).float() # bias for output layer\n",
    "\n",
    "    t = 0\n",
    "    loss = 1000\n",
    "    loss_all = []\n",
    "    while t < 40000:\n",
    "      # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "      # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "      # PyTorch to build a computational graph, allowing automatic computation of\n",
    "      # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "      # don't need to keep references to intermediate values.\n",
    "      y_pred = f(f(x.mm(w1).add(b1)).mm(w2).add(b2))\n",
    "\n",
    "      # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "      # is a Python number giving its value.\n",
    "      loss = (y_pred - y).pow(2).mean()\n",
    "      loss_all.append(loss.item())\n",
    "      # print(t, loss.item())\n",
    "\n",
    "      # Use autograd to compute the backward pass. This call will compute the\n",
    "      # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "      # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "      # of the loss with respect to w1 and w2 respectively.\n",
    "      loss.backward()\n",
    "\n",
    "      # Update weights using gradient descent. For this step we just want to mutate\n",
    "      # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "      # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "      # to prevent PyTorch from building a computational graph for the updates\n",
    "      with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        b1 -= learning_rate * b1.grad\n",
    "        b2 -= learning_rate * b2.grad\n",
    "\n",
    "        # Manually zero the gradients after running the backward pass\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "        t = t + 1\n",
    "    loss_all_lr.append(loss_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10,7))\n",
    "for i in range(len(loss_all_lr)):\n",
    "    plt.plot(np.log(np.array(loss_all_lr[i])))\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "#plt.ylim([0, 0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "185/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "94/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-23c5c0e6a876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Computing binary outputs using different thresholds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpercent_seqmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "# Computing binary outputs using different thresholds\n",
    "import difflib\n",
    "y_pred = y_pred.detach().numpy()\n",
    "threshold = 0.5\n",
    "percent_seqmatch = [0.99]\n",
    "behaviours_learnt = []\n",
    "for per in percent_seqmatch:\n",
    "    behaviour = 0\n",
    "\n",
    "    y_pred_binary = np.abs(np.round(y_pred+0.5-threshold))\n",
    "\n",
    "    for j in range(len(y_pred)):\n",
    "        s = difflib.SequenceMatcher(None, y[j],y_pred_binary[j])\n",
    "        if s.ratio() > per:\n",
    "            behaviour += 1\n",
    "    behaviours_learnt.append(behaviour/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the optimal learning rates, run 10 trials (with different random weights initializations) for each R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs_N200 = [2500, 1000, 650, 100, 100, 100, 100, 100, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0.9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0.8\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0.7\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0.6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0.5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0.3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "loss_all_R_trials = []\n",
    "y_pred_all_R_trials = []\n",
    "for f_ind, ff in enumerate(R_exps[:]):\n",
    "    print(ff)\n",
    "    R = int(N**ff)\n",
    "    learning_rate = lrs_N200[f_ind]\n",
    "    loss_all_trials = []\n",
    "    y_pred_all_trials = []\n",
    "    for trial in range(10):\n",
    "        print(trial)\n",
    "        # Create random Tensors for weights; setting requires_grad=True means that we\n",
    "        # want to compute gradients for these Tensors during the backward pass.\n",
    "        w1 = torch.randn(N, R, device=device, requires_grad=True).float()\n",
    "        w2 = torch.randn(R, M, device=device, requires_grad=True).float()\n",
    "\n",
    "        ## initialize tensor variables for bias terms \n",
    "        b1 = torch.randn(1, R, device=device, requires_grad=True).float()# bias for hidden layer\n",
    "        b2 = torch.randn(1, M, device=device, requires_grad=True).float() # bias for output layer\n",
    "\n",
    "        t = 0\n",
    "        loss = 1000\n",
    "        loss_all = []\n",
    "        while t < 40000:\n",
    "          # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "          # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "          # PyTorch to build a computational graph, allowing automatic computation of\n",
    "          # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "          # don't need to keep references to intermediate values.\n",
    "          y_pred = f(f(x.mm(w1).add(b1)).mm(w2).add(b2))\n",
    "\n",
    "          # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "          # is a Python number giving its value.\n",
    "          loss = (y_pred - y).pow(2).mean()\n",
    "          loss_all.append(loss.item())\n",
    "          # print(t, loss.item())\n",
    "\n",
    "          # Use autograd to compute the backward pass. This call will compute the\n",
    "          # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "          # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "          # of the loss with respect to w1 and w2 respectively.\n",
    "          loss.backward()\n",
    "\n",
    "          # Update weights using gradient descent. For this step we just want to mutate\n",
    "          # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "          # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "          # to prevent PyTorch from building a computational graph for the updates\n",
    "          with torch.no_grad():\n",
    "            w1 -= learning_rate * w1.grad\n",
    "            w2 -= learning_rate * w2.grad\n",
    "            b1 -= learning_rate * b1.grad\n",
    "            b2 -= learning_rate * b2.grad\n",
    "\n",
    "            # Manually zero the gradients after running the backward pass\n",
    "            w1.grad.zero_()\n",
    "            w2.grad.zero_()\n",
    "            b1.grad.zero_()\n",
    "            b2.grad.zero_()\n",
    "            t = t + 1\n",
    "        loss_all_trials.append(loss.detach().numpy())\n",
    "        y_pred_all_trials.append(y_pred.detach().numpy())\n",
    "    y_pred_all_R_trials.append(y_pred_all_trials)\n",
    "    loss_all_R_trials.append(loss_all_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Computing binary outputs using different thresholds\n",
    "import difflib\n",
    "behavior_all_R_trials = []\n",
    "for R_ind, y_pred_all_trials in enumerate(y_pred_all_R_trials):\n",
    "    print(R_ind)\n",
    "    behaviour_all_trials = []\n",
    "    for y_pred in y_pred_all_trials:\n",
    "        threshold = 0.5\n",
    "        percent_seqmatch = [0.99]\n",
    "        behaviours_learnt = []\n",
    "        for per in percent_seqmatch:\n",
    "            behaviour = 0\n",
    "\n",
    "            y_pred_binary = np.abs(np.round(y_pred+0.5-threshold))\n",
    "\n",
    "            for j in range(len(y_pred)):\n",
    "                s = difflib.SequenceMatcher(None, y[j],y_pred_binary[j])\n",
    "                if s.ratio() > per:\n",
    "                    behaviour += 1\n",
    "            behaviours_learnt.append(behaviour/N)\n",
    "        behaviour_all_trials.append(behaviour)\n",
    "    behavior_all_R_trials.append(behaviour_all_trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
